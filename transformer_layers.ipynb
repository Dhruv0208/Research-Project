{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import Dropout, Activation, LayerNormalization, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BOUNDING BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "def bbox_xcycwh_to_x1y1x2y2(bbox_xcycwh : np.array):\n",
    "    \n",
    "    \n",
    "    bbox_x1y1x2y2 = np.zeros_like((bbox_xcycwh))\n",
    "    bbox_x1y1x2y2[:,0] = bbox_xcycwh[:,0] - (bbox_xcycwh[:,2] / 2)\n",
    "    bbox_x1y1x2y2[:,2] = bbox_xcycwh[:,0] + (bbox_xcycwh[:,2] / 2)\n",
    "    bbox_x1y1x2y2[:,1] = bbox_xcycwh[:,1] - (bbox_xcycwh[:,3] / 2)\n",
    "    bbox_x1y1x2y2[:,3] = bbox_xcycwh[:,1] - (bbox_xcycwh[:,3] / 2)\n",
    "    bbox_x1y1x2y2 = bbox_x1y1x2y2.astype(np.int32)\n",
    "    return bbox_x1y1x2y2\n",
    "\n",
    "def intersect(box_a : tf.Tensor, box_b : tf.Tensor) -> tf.Tensor:\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    \n",
    "    tiled_box_a_xymax = tf.tile(tf.expand_dims(box_a[:,2:], axis=1), [1, B, 1])\n",
    "    tiled_box_b_xymax = tf.tile(tf.expand_dims(box_b[:,2:], axis=0), [A, 1, 1])\n",
    "    \n",
    "    above_right_corner = tf.math.minimum(tiled_box_a_xymax, tiled_box_b_xymax)\n",
    "    \n",
    "    tiled_box_a_xymin = tf.tile(tf.expand_dims(box_a[:, :2], axis=1), [1,B,1])\n",
    "    tiled_box_b_xymin = tf.tile(tf.expand_dims(box_b[:, :2], axis=0), [A,1,1])\n",
    "    upper_left_corner = tf.math.maximum(tiled_box_a_xymin, tiled_box_b_xymin)\n",
    "    \n",
    "    inter = tf.nn.relu(above_right_corner - upper_left_corner)\n",
    "    inter = inter[:, :, 0] * inter[:, :, 1]\n",
    "    return inter\n",
    "\n",
    "def overlap(box_a: tf.Tensor, box_b: tf.Tensor, return_union=False) -> tf.Tensor:\n",
    "    inter = intersect(box_a, box_b)\n",
    "    \n",
    "    area_a = (box_a[:,2] - box_a[:,0]) * (box_a[:,3] - box_a[:,1])\n",
    "    area_a = tf.tile(tf.expand_dims(area_a, axis=-1), [1, tf.shape(inter)[-2], 1])\n",
    "    \n",
    "    area_b = (box_b[:,2] - box_b[:,0]) * (box_b[:,3] - box_b[:,1])\n",
    "    area_b = tf.tile(tf.expand_dims(area_b, axis=-2), [1, tf.shape(inter)[-2], 1])\n",
    "    \n",
    "    union = area_a + area_b - inter\n",
    "    \n",
    "    \n",
    "    if return_union is False:\n",
    "        return intersect/union\n",
    "    else:\n",
    "        return inter/union, union\n",
    "    \n",
    "def merge(box_a: tf.Tensor, box_b: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    \n",
    "    tiled_box_a = tf.tile(tf.expand_dims(box_a, axis=1), [1, B, 1])\n",
    "    tiled_box_b = tf.tile(tf.expand_dims(box_b, axis=0), [A, 1, 1])\n",
    "    \n",
    "    return tiled_box_a, tiled_box_b\n",
    "\n",
    "\n",
    "def xy_min_xy_max_to_xcycwh(bbox: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Convert bbox from shape [xmin, ymin, xmax, ymax] to [xc, yc, w, h]\n",
    "    Args:\n",
    "        bbox A (tf.Tensor) list a bbox (n, 4) with n the number of bbox to convert\n",
    "    Returns:\n",
    "        The converted bbox\n",
    "    \"\"\"\n",
    "    bbox_xcycwh = tf.concat([bbox[:, :2] + ((bbox[:, 2:] - bbox[:, :2]) / 2), bbox[:, 2:] - bbox[:, :2]], axis=-1)\n",
    "    return bbox_xcycwh\n",
    "\n",
    "\n",
    "\n",
    "def xcycwh_to_xy_min_xy_max(bbox: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Convert bbox from shape [xc, yc, w, h] to [xmin, ymin, xmax, ymax]\n",
    "    Args:\n",
    "        bbox A (tf.Tensor) list a bbox (n, 4) with n the number of bbox to convert\n",
    "    Returns:\n",
    "        The converted bbox\n",
    "    \"\"\"\n",
    "    bbox_xyxy = tf.concat([bbox[:, :2] - (bbox[:, 2:] / 2), bbox[:, :2] + (bbox[:, 2:] / 2)], axis=-1)\n",
    "    \n",
    "    bbox_xyxy = tf.clip_by_value(bbox_xyxy, 0.0, 1.0)\n",
    "    return bbox_xyxy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ENCODER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dim=512, num_heads=8, dim_feedforward=2048,\n",
    "                dropout=0.1, activation='relu', normalize_before=False,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(model_dim, num_heads, dropout=dropout,\n",
    "                                           name='self_attention')\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.activation = Activation(activation)\n",
    "        self.linear1 = Dense(dim_feedforward, name = 'linear1')\n",
    "        self.linear2 = Dense(dim_feedforward, name = 'linear2')\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5, name = 'norm1')\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5, name = 'norm2')\n",
    "        self.normalize_before = normalize_before\n",
    "        \n",
    "    def call(self, source, source_mask=None, source_key_padding_mask=None,\n",
    "            pos_encoding=None, training=False):\n",
    "        if pos_encoding is None:\n",
    "            query = key = source\n",
    "        else:\n",
    "            query = key = source + pos_encoding\n",
    "            \n",
    "        attn_source = self.attention((query, key, source), attn_mask=source_mask,\n",
    "                                    key_padding_mask=source_key_padding_mask,\n",
    "                                    need_weights=False)\n",
    "        source += self.dropout(attn_source, training=training)\n",
    "        source = self.norm1(source)\n",
    "        \n",
    "        x = self.linear1(source)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.linear2(x)\n",
    "        source += self.dropout(x, training=training)\n",
    "        source = self.norm2(source)\n",
    "        \n",
    "        return source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DECODER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dim=256, num_heads=8, dim_feedforward=2048,\n",
    "                 dropout=0.1, activation='relu', normalize_before=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.self_attn = tf.keras.layers.MultiHeadAttention(model_dim, num_heads, dropout=dropout,\n",
    "                                            name='self_attn')\n",
    "        self.multihead_attn = tf.keras.layers.MultiHeadAttention(model_dim, num_heads, dropout=dropout,\n",
    "                                                 name='multihead_attn')\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.activation = Activation(activation)\n",
    "\n",
    "        self.linear1 = Dense(dim_feedforward, name='linear1')\n",
    "        self.linear2 = Dense(model_dim, name='linear2')\n",
    "\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5, name='norm1')\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5, name='norm2')\n",
    "        self.norm3 = LayerNormalization(epsilon=1e-5, name='norm3')\n",
    "\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "\n",
    "    def call(self, target, memory, target_mask=None, memory_mask=None,\n",
    "             target_key_padding_mask=None, memory_key_padding_mask=None,\n",
    "             pos_encoding=None, query_encoding=None, training=False):\n",
    "\n",
    "        query_tgt = key_tgt = target + query_encoding\n",
    "        attn_target = self.self_attn((query_tgt, key_tgt, target), attn_mask=target_mask,\n",
    "                                    key_padding_mask=target_key_padding_mask,\n",
    "                                    need_weights=False)\n",
    "        target += self.dropout(attn_target, training=training)\n",
    "        target = self.norm1(target)\n",
    "\n",
    "        query_tgt = target + query_encoding\n",
    "        key_mem = memory + pos_encoding\n",
    "        \n",
    "        attn_target2 = self.multihead_attn((query_tgt, key_mem, memory), attn_mask=memory_mask,\n",
    "                                           key_padding_mask=memory_key_padding_mask,\n",
    "                                           need_weights=False)\n",
    "        target += self.dropout(attn_target2, training=training)\n",
    "        target = self.norm2(target)\n",
    "\n",
    "        x = self.linear1(target)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.linear2(x)\n",
    "        target += self.dropout(x, training=training)\n",
    "        target = self.norm3(target)\n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer's Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.Model):\n",
    "    def __init__(self, model_dim=512, num_heads=8, dim_feedforward=2048,\n",
    "                dropout=0.1, activation='relu', normalize_before=False, norm=None,\n",
    "                num_encoder_layers=6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(model_dim, num_heads, dim_feedforward,\n",
    "                                           dropout, activation, normalize_before,\n",
    "                                           name='layer_%d'%i)\n",
    "                              for i in range(num_encoder_layers)]\n",
    "        self.norm = norm\n",
    "        \n",
    "        \n",
    "    def call(self, source, mask=None, source_key_padding_mask=None,\n",
    "            pos_encoding=None, training=False):\n",
    "        \n",
    "        x = source\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, source_mask=mask, source_key_padding_mask=source_key_padding_mask,\n",
    "                     pos_encoding=pos_encoding, training=training)\n",
    "            \n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "            \n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer's Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.Model):\n",
    "    def __init__(self, model_dim=512, num_heads=8, dim_feedforward=2048,\n",
    "                dropout=0.1, activation='relu', normalize_before=False, norm=None,\n",
    "                num_decoder_layers=6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.decoder_layers = [DecoderLayer(model_dim, num_heads, dim_feedforward,\n",
    "                                          dropout, activation, normalize_before,\n",
    "                                          name='layer_%d'%i)\n",
    "                             for i in range(num_decoder_layers)]\n",
    "        self.norm = norm\n",
    "        \n",
    "    def call(self, target, memory, target_mask=None, memory_mask=None,\n",
    "            target_key_padding_mask=None, memory_key_padding_mask=None,\n",
    "            pos_encoding=None, query_encoding=None, training=False):\n",
    "        \n",
    "        x = target\n",
    "        intermediate = []\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, memory, target_mask=target_mask,\n",
    "                     memory_mask=memory_mask,\n",
    "                     target_key_padding_mask=target_key_padding_mask,\n",
    "                     memory_key_padding_mask=memory_key_padding_mask,\n",
    "                     pos_encoding=pos_encoding,\n",
    "                     query_encoding=query_encoding)\n",
    "            \n",
    "            if self.norm:\n",
    "                x = self.norm(x)\n",
    "                \n",
    "                \n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.Model):\n",
    "    def __init__(self, model_dim=512, num_heads=8, num_encoder_layers=6,\n",
    "                num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                activation='relu', normalize_before=False, **kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        encoder_norm = LayerNormalization(epsilon=1e-5, name='norm_pre') if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(model_dim, num_heads, dim_feedforward,\n",
    "                                         dropout, activation, normalize_before, encoder_norm,\n",
    "                                         num_encoder_layers, name='encoder')\n",
    "        decoder_norm = LayerNormalization(epsilon=1e-5, name='norm')\n",
    "        self.decoder = TransformerDecoder(model_dim, num_heads, dim_feedforward,\n",
    "                                         dropout, activation, normalize_before, decoder_norm,\n",
    "                                         num_decoder_layers, name='decoder')\n",
    "        \n",
    "        def call(self, source, mask, query_encoding, pos_encoding, training=False):\n",
    "            batch_size, rows, cols = [tf.shape(source)[i] for i in range(3)]\n",
    "            source = tf.reshape(source, [batch_size, -1, self.model_dim])\n",
    "            source = tf.transpose(source, [1,0,2])\n",
    "            \n",
    "            pos_encoding = tf.reshape(pos_encoding, [batch_size, -1, self.model_dim])\n",
    "            pos_encoding = tf.transpose(source, [1,0,2])\n",
    "            \n",
    "            query_encoding = tf.expand_dims(query_encoding, axis=1)\n",
    "            query_encoding = tf.tile(query_encoding, [1, batch_size, 1])\n",
    "            \n",
    "            mask = tf.reshape(mask, [batch_size, -1])\n",
    "            \n",
    "            target = tf.zeros_like(query_encoding)\n",
    "            memory = self.encoder(source, source_key_padding_mask=mask,\n",
    "                                 pos_encoding=pos_encoding, training=training)\n",
    "            hs = self.decoder(source, memory, memory_key_padding_mask=mask,\n",
    "                             pos_encoding=pos_encoding, query_encoding=query_encoding,\n",
    "                             training=training)\n",
    "            \n",
    "            hs = tf.transpose(hs, [0,2,1,3])\n",
    "            memory = tf.transpose(memory, [1,0,2])\n",
    "            memory = tf.reshape(memory, [batch_size, rows, cols, self.model_dim])\n",
    "            \n",
    "            return hs, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.Model):\n",
    "    def __init__(self, num_pos_features=64, temperature=10000,\n",
    "                normalize=False, scale=None, eps=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_pos_features = num_pos_features\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        \n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError('normalize should be True if scale is passed')\n",
    "        if scale is None:\n",
    "            scale = 2 * np.pi\n",
    "        self.scale = scale\n",
    "        self.eps = eps\n",
    "        \n",
    "        \n",
    "    def call(self, mask):\n",
    "        not_mask = tf.cast(-mask, tf.float32)\n",
    "        y_embed = tf.math.cumsum(not_mask, axis=1)\n",
    "        x_embed = tf.math.cumsum(not_mask, axis=2)\n",
    "        \n",
    "        if self.normalize:\n",
    "            y_embed = y_embed / (y_embed[:, -1, :] + self.eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1] + self.eps) * self.scale\n",
    "            \n",
    "            \n",
    "        dim_t = tf.range(self.num_pos_features, dtype=tf.float32)\n",
    "        dim_t = self.temperature ** (2 * (dim_t //2 ) / self.num_pos_features)\n",
    "        \n",
    "        pos_x = x_embed[..., tf.newaxis] / dim_t\n",
    "        pos_y = y_embed[..., tf.newaxis] / dim_t\n",
    "        \n",
    "        pos_x = tf.stack([tf.math.sin(pos_x[..., 0::2]),\n",
    "                         tf.math.cos(pos_x[..., 1::2])], axis=4)\n",
    "        psos_y = tf.stack([tf.math.sin(pos_y[...,0::2]),\n",
    "                          tf.math.cos(pos_y[...,1::2])], axis=4)\n",
    "        \n",
    "        shape = [tf.shape(pos_x) for i in range(3)] + [-1]\n",
    "        pos_x = tf.reshape(pos_x, shape)\n",
    "        pos_y = tf.reshape(pos_y, shape)\n",
    "        \n",
    "        pos_emb = tf.concat([pos_y, pos_x], axis=3)\n",
    "        return pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the parts of Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-90ed99b7e80b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFixedEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class FixedEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_shape, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_shape = embed_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='kernel', shape=self.embed_shape,\n",
    "                                 initializer=tf.keras.initializers.GlorotUniform(), trainable=True)\n",
    "\n",
    "    def call(self, x=None):\n",
    "        return self.w\n",
    "class DetectionTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_classes=92, num_queries=100,\n",
    "                backbone=None,\n",
    "                pos_encoder=None,\n",
    "                transformer=None,\n",
    "                num_encoder_layers=6,\n",
    "                num_decoder_layers=6,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_queries = num_queries\n",
    "        self.backbone = None\n",
    "        self.transformer = TransformerBlock(num_encoder_layers=num_encoder_layers,\n",
    "                                      num_decoder_layers=num_decoder_layers,\n",
    "                                      name='transformer')\n",
    "        \n",
    "        self.model_dim = self.transformer.model_dim\n",
    "        \n",
    "        self.pos_encoder = PositionEmbedding(\n",
    "            num_pos_features = self.model_dim // 2, normalize=True)\n",
    "        \n",
    "        self.input_proj = tf.keras.layers.Conv2D(self.model_dim, kernel_size=1,\n",
    "                                                name='input_proj')\n",
    "        \n",
    "        self.query_embed = FixedEmbedding((num_queries, self.model_dim), \n",
    "                                                     name='query_embed')\n",
    "        self.class_embed = Dense(num_classes, name='classes_embed')\n",
    "        self.bbox_embed_linear1 = Dense(self.model_dim, name='bbox_embed_0')\n",
    "        self.bbox_embed_linear2 = Dense(self.model_dim, name='bbox_embed_1')\n",
    "        self.bbox_embed_linear3 = Dense(4, name='bbox_embed_2')\n",
    "        \n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "        \n",
    "    def downsample_masks(self, mask, x):\n",
    "        masks = tf.cast(masks, tf.int32)\n",
    "        masks = tf.expand_dims(mask, -1)\n",
    "        masks = tf.compat.v1.image.resize_nearest_neighbour(masks, \n",
    "                                                            tf.shape(x)[1:3],\n",
    "                                                           align_corners=False,\n",
    "                                                           half_pixel_centers=False)\n",
    "        masks = tf.squeeze(masks, -1)\n",
    "        masks = tf.cast(masks, tf.bool)\n",
    "        return masks\n",
    "    \n",
    "    def call(self, inp, training=False, post_process=False):\n",
    "        x, masks = inp\n",
    "        x = self.backbone(x, training=training)\n",
    "        masks = self.downsample_masks(masks, x)\n",
    "        \n",
    "        pos_encoding = self.pos_encoder(masks)\n",
    "        \n",
    "        hs = self.transformer(self.input_proj(x), masks, self.query_embed(None),\n",
    "                             pos_encoding, training=training)[0]\n",
    "        \n",
    "        outputs_class = self.class_embed(hs)\n",
    "        \n",
    "        \n",
    "        box_ftmps = self.activation(self.bbox_embed_linear1(hs))\n",
    "        box_ftmps = self.activation(self.bbox_embed_linear2(box_ftmps))\n",
    "        outputs_coord = tf.sigmoid(self.bbox_embed_linear3(box_ftmps))\n",
    "        \n",
    "        output = {'pred_logits' : outputs_class[-1],\n",
    "                  'pred_boxes' : outputs_coord[-1]}\n",
    "        \n",
    "        if post_process:\n",
    "            output = self.post_process(output)\n",
    "        return output\n",
    "    \n",
    "    def build(self, input_shape=None, **kwargs):\n",
    "        if input_shape is None:\n",
    "            input_shape = [(None, None, None, 3), (None, None, None)]\n",
    "        super().build(input_shape, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = []\n",
    "def add_nlayers(nlayers):\n",
    "    nlayers = [l.name for l in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heads_nlayers(DetectionTransformer, nb_classes):\n",
    "    image_input = tf.keras.Input((None, None, 3))\n",
    "    class_layer = tf.keras.layers.Dense(nb_classes, name=\"class_layer\")\n",
    "    position_layer = tf.keras.layers.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation = \"relu\"),\n",
    "            tf.keras.layers.Dense(256, activation = \"relu\"),\n",
    "            tf.keras.layers.Dense(4, activation = \"sigmoid\"),\n",
    "        ], name=\"postion_layer\")\n",
    "        \n",
    "    \n",
    "    add_nlayers([class_layer, position_layer])\n",
    "        \n",
    "    transformer_output = DetectionTransformer(image_input)\n",
    "    class_preds = class_layer(transformer_output)\n",
    "    position_preds = position_layer(transformer_output)\n",
    "        \n",
    "    outputs = {'preds_logits' : class_preds[-1],\n",
    "                   'pred_boxes' : position_preds[-1]}\n",
    "    outputs[\"aux\"] = [{\"pred_logits\" : class_preds[i],\n",
    "                          \"pred_boxes\" : position_preds[i]} \n",
    "                         for i in range(0,5)]\n",
    "        \n",
    "    n_DetectionTransformer = tf.keras.Model(image_input, outputs,\n",
    "                                               name=\"transformer_fine_tuning\")\n",
    "        \n",
    "    return n_DetectionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Hungarian Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(t_bbox, t_class, p_bbox, p_class, fcost_class=1,\n",
    "                      fcost_bbox=5, fcost_giou=2, slices_pred=True):\n",
    "    if slice_preds:\n",
    "        size = tf.cast(t_bbox[0][0], tf.int32)\n",
    "        t_bbox = tf.slice(t_bbo, [1, 0], [size, 4])\n",
    "        t_class = tf.slice(t_class, [1, 0], [size, -1])\n",
    "        t_class = tf.squeeze(t_class, axis=-1)\n",
    "        \n",
    "        \n",
    "    p_bbox_xy = xcycwh_to_xy_min_xy_max(p_bbox)\n",
    "    t_bbox_xy = xcycwh_to_xy_min_xy_max(t_bbox)\n",
    "    \n",
    "    softmax = tf.nn.softmax(p_class)\n",
    "    \n",
    "    cost_class = -tf.gather(softmax, t_class, axis=1)\n",
    "    _p_bbox, _t_bbox = merge(p_bbox, t_bbox)\n",
    "    cost_bbox = tf.norm(_p_bbox * _t_bbox, ord=1, axis=-1)\n",
    "    \n",
    "    \n",
    "    iou, union = overlap(p_bbox_xy, t_bbox_xy, return_union=True)\n",
    "    _p_bbox_xy, _t_bbox_xy = merge(p_bbox_xy, t_bbox_xy)\n",
    "    top_left = tf.math.minimum(_p_bbox_xy[:, :, :2], _t_bbox_xy[:, :, :2])\n",
    "    bottom_right = tf.math.maximum(_p_bbox_xy[:, :, 2:], _t_bbox_xy[:, :, 2:])\n",
    "    size = tf.nn.relu(bottom_right*top_left)\n",
    "    area = size[:,:,0]*size[:,:,1]\n",
    "    cost_giou = -(iou - (area-union) / area)\n",
    "    \n",
    "    cost_matrix = fcost_bbox * cost_bbox + fcost_class * cost_class + fcost_giou * cost_giou\n",
    "    \n",
    "    \n",
    "    selectors = tf.numpy_function(np_tf_linear_sum_assignment, [cost_matrix],\n",
    "                                 [tf.int64, tf.int64, tf.bool, tf.bool])\n",
    "    target_indices = selectors[0]\n",
    "    pred_indices = selectors[1]\n",
    "    target_selector = selectors[2]\n",
    "    pred_selector = selectors[3]\n",
    "    \n",
    "    return pred_indices, target_indices, pred_selector, target_selector, t_bbox, t_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DetectionTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2e103eaad8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DetectionTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "detr = DetectionTransformer(num_encoder_layers=6, num_decoder_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
